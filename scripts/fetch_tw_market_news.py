#!/usr/bin/env python3
"""
å°è‚¡æ™‚äº‹åˆ†æå·¥å…· v4.0ï¼ˆ2026-01-22 å‡ç´šï¼‰
æ•´åˆå¤šä¾†æºå‹•æ…‹ç²å–å°è‚¡è³‡è¨Šï¼š

ğŸ”´ é‡è¦ä¾†æºï¼ˆv4.0 æ–°å¢ï¼‰ï¼š
1. MOPS å…¬é–‹è³‡è¨Šè§€æ¸¬ç«™ - é‡å¤§è¨Šæ¯ï¼ˆä½µè³¼ã€è¨‚å–®ã€æ³•èªªæœƒï¼‰â¬…ï¸ æœ€é‡è¦ï¼
2. Yahoo è‚¡å¸‚æ–°è - å³æ™‚æ–°è

ğŸ“° åŸæœ‰ä¾†æºï¼š
3. è­‰äº¤æ‰€æ–°è API - å®˜æ–¹å…¬å‘Š
4. è­‰äº¤æ‰€å…¬å‘Š API - é‡å¤§è¨Šæ¯
5. é‰…äº¨ç¶²æ–°è API - è²¡ç¶“æ–°è + é—œéµå­—
6. ç¶“æ¿Ÿæ—¥å ± RSS - è£œå……æ–°è

æ‰€æœ‰è³‡æ–™çš†å¾å³æ™‚ä¾†æºå‹•æ…‹ç²å–ï¼Œç„¡ç¡¬ç·¨ç¢¼

ç”¨æ³•ï¼š
  python3 scripts/fetch_tw_market_news.py
"""

import sys
import io

# Windows ç’°å¢ƒ stdout ç·¨ç¢¼ä¿®æ­£ï¼ˆé¿å… emoji è¼¸å‡ºæ™‚ cp950 å ±éŒ¯ï¼‰
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')

import requests
import urllib3

# å¿½ç•¥ SSL è­¦å‘Šï¼ˆå…¬å¸ç¶²è·¯ç’°å¢ƒéœ€è¦ï¼‰
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
from datetime import datetime
from bs4 import BeautifulSoup
import json
import re
import xml.etree.ElementTree as ET

# å¿½ç•¥ SSL è­¦å‘Š
import warnings
warnings.filterwarnings('ignore')

# é‡é»è‚¡ç¥¨åç¨±å°ç…§ï¼ˆç”¨æ–¼æ¨™è¨»ï¼‰
STOCK_NAMES = {
    # åŠå°é«”
    '2330': 'å°ç©é›»', '2303': 'è¯é›»', '2454': 'è¯ç™¼ç§‘', '3711': 'æ—¥æœˆå…‰æŠ•æ§',
    '2379': 'ç‘æ˜±', '3034': 'è¯è© ', '6770': 'åŠ›ç©é›»', '2408': 'å—äºç§‘',
    '2344': 'è¯é‚¦é›»', '2337': 'æ—ºå®',
    # é›»å­
    '2317': 'é´»æµ·', '2382': 'å»£é”', '3231': 'ç·¯å‰µ', '2357': 'è¯ç¢©',
    '3037': 'æ¬£èˆˆ', '3189': 'æ™¯ç¢©', '8046': 'å—é›»', '6239': 'åŠ›æˆ',
    # é‡‘è
    '2881': 'å¯Œé‚¦é‡‘', '2882': 'åœ‹æ³°é‡‘', '2883': 'å‡±åŸºé‡‘', '2884': 'ç‰å±±é‡‘',
    '2885': 'å…ƒå¤§é‡‘', '2886': 'å…†è±é‡‘', '2887': 'å°æ–°é‡‘', '2891': 'ä¸­ä¿¡é‡‘',
    '2890': 'æ°¸è±é‡‘', '2892': 'ç¬¬ä¸€é‡‘',
    # å‚³ç”¢
    '1301': 'å°å¡‘', '1303': 'å—äº', '1326': 'å°åŒ–', '2002': 'ä¸­é‹¼',
    '2603': 'é•·æ¦®', '2609': 'é™½æ˜', '2615': 'è¬æµ·',
    # å…¶ä»–
    '3481': 'ç¾¤å‰µ', '2409': 'å‹é”', '8150': 'å—èŒ‚'
}


def get_twse_news():
    """å¾è­‰äº¤æ‰€ç²å–å®˜æ–¹æ–°è"""
    news = []
    url = 'https://www.twse.com.tw/rwd/zh/news/newsList?limit=15'

    try:
        r = requests.get(url, timeout=10, verify=False)
        if r.status_code == 200:
            data = r.json()
            if data.get('stat') == 'ok' and 'data' in data:
                for item in data['data'][:10]:
                    if len(item) >= 3:
                        news.append({
                            'title': item[1][:80] if len(item) > 1 else '',
                            'date': item[2] if len(item) > 2 else '',
                            'source': 'è­‰äº¤æ‰€',
                            'type': 'official'
                        })
    except Exception as e:
        print(f"  âš ï¸ è­‰äº¤æ‰€æ–°è: {e}")

    return news


def get_twse_announcements():
    """å¾è­‰äº¤æ‰€ç²å–å…¬å‘Š"""
    announcements = []
    url = 'https://www.twse.com.tw/rwd/zh/announcement/announcement?limit=10'

    try:
        r = requests.get(url, timeout=10, verify=False)
        if r.status_code == 200:
            data = r.json()
            if data.get('stat') == 'ok' and 'data' in data:
                for item in data['data'][:8]:
                    if len(item) >= 4:
                        announcements.append({
                            'date': item[1] if len(item) > 1 else '',
                            'doc_no': item[2] if len(item) > 2 else '',
                            'subject': item[3][:100] if len(item) > 3 else '',
                            'source': 'è­‰äº¤æ‰€å…¬å‘Š'
                        })
    except Exception as e:
        print(f"  âš ï¸ è­‰äº¤æ‰€å…¬å‘Š: {e}")

    return announcements


def get_cnyes_news():
    """å¾é‰…äº¨ç¶²ç²å–è²¡ç¶“æ–°è"""
    news = []
    url = 'https://news.cnyes.com/api/v3/news/category/tw_stock'

    try:
        headers = {'User-Agent': 'Mozilla/5.0', 'Accept': 'application/json'}
        params = {'page': 1, 'limit': 20}

        r = requests.get(url, headers=headers, params=params, timeout=10, verify=False)

        if r.status_code == 200:
            data = r.json()
            if 'items' in data and 'data' in data['items']:
                for item in data['items']['data'][:15]:
                    publish_time = item.get('publishAt', 0)
                    date_str = datetime.fromtimestamp(publish_time).strftime('%Y-%m-%d %H:%M') if publish_time else ''

                    news.append({
                        'title': item.get('title', ''),
                        'date': date_str,
                        'summary': item.get('summary', '')[:100],
                        'keywords': item.get('keyword', []),
                        'source': 'é‰…äº¨ç¶²'
                    })
    except Exception as e:
        print(f"  âš ï¸ é‰…äº¨ç¶²æ–°è: {e}")

    return news


def get_udn_rss():
    """å¾ç¶“æ¿Ÿæ—¥å ± RSS ç²å–æ–°è"""
    news = []
    url = 'https://money.udn.com/rssfeed/news/1001/5590'

    try:
        r = requests.get(url, timeout=10, verify=False)
        if r.status_code == 200:
            root = ET.fromstring(r.content)

            for item in root.findall('.//item')[:10]:
                title = item.find('title')
                pub_date = item.find('pubDate')

                if title is not None:
                    news.append({
                        'title': title.text[:80] if title.text else '',
                        'date': pub_date.text[:25] if pub_date is not None and pub_date.text else '',
                        'source': 'ç¶“æ¿Ÿæ—¥å ±'
                    })
    except Exception as e:
        print(f"  âš ï¸ ç¶“æ¿Ÿæ—¥å ± RSS: {e}")

    return news


def get_mops_announcements():
    """
    å¾ MOPS å…¬é–‹è³‡è¨Šè§€æ¸¬ç«™ç²å–é‡å¤§è¨Šæ¯ï¼ˆv4.0 æ–°å¢ï¼‰
    é€™æ˜¯æœ€é‡è¦çš„è³‡æ–™ä¾†æºï¼šä½µè³¼ã€è¨‚å–®ã€æ“´ç”¢ã€æ¸›è³‡ã€æ³•èªªæœƒå…¬å‘Š
    """
    announcements = []
    today = datetime.now().strftime('%Y%m%d')

    # MOPS é‡å¤§è¨Šæ¯ API
    url = 'https://mops.twse.com.tw/mops/web/ajax_t05st01'

    try:
        headers = {
            'User-Agent': 'Mozilla/5.0',
            'Content-Type': 'application/x-www-form-urlencoded'
        }

        # æŸ¥è©¢ä»Šæ—¥é‡å¤§è¨Šæ¯
        data = {
            'encodeURIComponent': '1',
            'step': '1',
            'firstin': '1',
            'off': '1',
            'TYPEK': 'all',
            'year': str(int(today[:4]) - 1911),  # æ°‘åœ‹å¹´
            'month': today[4:6],
            'day': today[6:8]
        }

        r = requests.post(url, headers=headers, data=data, timeout=15, verify=False)

        if r.status_code == 200:
            soup = BeautifulSoup(r.text, 'html.parser')

            # æ‰¾åˆ°è³‡æ–™è¡¨æ ¼
            tables = soup.find_all('table', class_='hasBorder')

            for table in tables:
                rows = table.find_all('tr')
                for row in rows[1:]:  # è·³éè¡¨é ­
                    cols = row.find_all('td')
                    if len(cols) >= 4:
                        stock_code = cols[0].get_text(strip=True)
                        stock_name = cols[1].get_text(strip=True)
                        subject = cols[2].get_text(strip=True)
                        pub_date = cols[3].get_text(strip=True) if len(cols) > 3 else ''

                        # éæ¿¾é‡é»è¨Šæ¯
                        important_keywords = ['æ³•èªª', 'è¨‚å–®', 'ç‡Ÿæ”¶', 'åˆä½µ', 'æ”¶è³¼',
                                            'æ“´ç”¢', 'æ¸›è³‡', 'å¢è³‡', 'è‚¡åˆ©', 'é…æ¯',
                                            'è²¡å ±', 'è‘£äº‹æœƒ', 'é‡å¤§', 'ç°½ç´„', 'å‡ºè²¨']

                        is_important = any(kw in subject for kw in important_keywords)

                        if stock_code and subject:
                            announcements.append({
                                'stock_code': stock_code,
                                'stock_name': stock_name,
                                'subject': subject[:80],
                                'date': pub_date,
                                'source': 'MOPS',
                                'is_important': is_important
                            })

    except Exception as e:
        print(f"  âš ï¸ MOPS é‡å¤§è¨Šæ¯: {e}")

    return announcements[:20]  # æœ€å¤š20å‰‡


def get_yahoo_tw_news():
    """
    å¾ Yahoo è‚¡å¸‚ç²å–å³æ™‚æ–°èï¼ˆv4.0 æ–°å¢ï¼‰
    """
    news = []
    url = 'https://tw.stock.yahoo.com/rss?category=tw-market'

    try:
        r = requests.get(url, timeout=10, verify=False)
        if r.status_code == 200:
            root = ET.fromstring(r.content)

            for item in root.findall('.//item')[:12]:
                title = item.find('title')
                pub_date = item.find('pubDate')
                description = item.find('description')

                if title is not None and title.text:
                    news.append({
                        'title': title.text[:80],
                        'date': pub_date.text[:25] if pub_date is not None and pub_date.text else '',
                        'summary': description.text[:100] if description is not None and description.text else '',
                        'source': 'Yahooè‚¡å¸‚'
                    })
    except Exception as e:
        print(f"  âš ï¸ Yahooè‚¡å¸‚æ–°è: {e}")

    return news


def detect_conferences(all_news):
    """å¾æ–°èä¸­åµæ¸¬æ³•èªªæœƒè³‡è¨Š"""
    conferences = []
    conf_keywords = ['æ³•èªª', 'æ³•äººèªªæ˜', 'æ¥­ç¸¾ç™¼è¡¨', 'æ¥­ç¸¾èªªæ˜æœƒ']

    seen = set()
    for news in all_news:
        title = news.get('title', '')

        if any(kw in title for kw in conf_keywords):
            # å˜—è©¦æå–è‚¡ç¥¨åç¨±
            for code, name in STOCK_NAMES.items():
                if name in title and code not in seen:
                    seen.add(code)
                    conferences.append({
                        'stock_code': code,
                        'stock_name': name,
                        'title': title[:60],
                        'date': news.get('date', ''),
                        'source': news.get('source', '')
                    })
                    break

    return conferences


def detect_events(all_news):
    """å¾æ–°èä¸­åµæ¸¬é‡è¦äº‹ä»¶"""
    events = []

    event_keywords = {
        'Fed': ('Fedåˆ©ç‡æ±ºè­°', 'é‡‘èè‚¡', 'âš ï¸ é«˜'),
        'å‡æ¯': ('å¤®è¡Œå‡æ¯', 'é‡‘èè‚¡', 'âš ï¸ é«˜'),
        'é™æ¯': ('å¤®è¡Œé™æ¯', 'é‡‘èè‚¡', 'âš ï¸ é«˜'),
        'CPI': ('CPIå…¬å¸ƒ', 'å…¨å¸‚å ´', 'ğŸ“Œ æ³¨æ„'),
        'éè¾²': ('éè¾²å°±æ¥­æ•¸æ“š', 'å…¨å¸‚å ´', 'ğŸ“Œ æ³¨æ„'),
        'è²¡å ±': ('è²¡å ±å­£', 'å…¨å¸‚å ´', 'ğŸ“Œ æ³¨æ„'),
        'é™¤æ¯': ('é™¤æ¬Šæ¯æ—ºå­£', 'é«˜æ¯è‚¡', 'ğŸ“Œ æ³¨æ„'),
    }

    detected = set()
    for news in all_news:
        title = news.get('title', '') + news.get('summary', '')

        for keyword, (event_name, stocks, impact) in event_keywords.items():
            if keyword in title and event_name not in detected:
                detected.add(event_name)
                events.append({
                    'event': event_name,
                    'stocks': stocks,
                    'impact': impact,
                    'source_title': news.get('title', '')[:40],
                    'date': news.get('date', '')
                })

    return events[:5]


def analyze_hot_topics(all_news):
    """åˆ†æç†±é–€é¡Œæ"""
    keywords = {}

    hot_topics = {
        'AI': ['AI', 'äººå·¥æ™ºæ…§', 'GPU', 'è¼é”', 'NVIDIA', 'ChatGPT', 'Blackwell'],
        'è¨˜æ†¶é«”': ['è¨˜æ†¶é«”', 'DRAM', 'HBM', 'NAND', 'ç¾å…‰', 'Micron'],
        'åŠå°é«”': ['åŠå°é«”', 'æ™¶ç‰‡', 'æ™¶åœ“', 'CoWoS', 'å…ˆé€²å°è£', 'å°ç©é›»'],
        'é¢æ¿': ['é¢æ¿', 'LCD', 'OLED', 'é¡¯ç¤ºå™¨', 'ç¾¤å‰µ', 'å‹é”'],
        'é›»å‹•è»Š': ['é›»å‹•è»Š', 'EV', 'ç‰¹æ–¯æ‹‰', 'é›»æ± ', 'å……é›»'],
        'èˆªé‹': ['èˆªé‹', 'é‹åƒ¹', 'è²¨æ«ƒ', 'BDI', 'é•·æ¦®', 'é™½æ˜'],
        'é‡‘è': ['å‡æ¯', 'é™æ¯', 'Fed', 'å¤®è¡Œ', 'é‡‘è', 'éŠ€è¡Œ', 'å£½éšª'],
        'ä½µè³¼': ['ä½µè³¼', 'æ”¶è³¼', 'åˆä½µ', 'è‚¡æ¬Š'],
        'ETF': ['ETF', 'æˆåˆ†è‚¡', 'èª¿æ•´', 'ç´å…¥'],
    }

    for news in all_news:
        text = news.get('title', '') + news.get('summary', '')
        # ä¹Ÿä½¿ç”¨é‰…äº¨ç¶²æä¾›çš„é—œéµå­—
        if news.get('keywords'):
            text += ' '.join(news.get('keywords', []))

        for topic, kws in hot_topics.items():
            for kw in kws:
                if kw in text:
                    keywords[topic] = keywords.get(topic, 0) + 1
                    break

    return sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:6]


def print_summary(twse_news, twse_announcements, cnyes_news, udn_news, conferences, events, hot_topics, mops_announcements=None, yahoo_news=None):
    """è¼¸å‡ºæ™‚äº‹æ‘˜è¦ï¼ˆv4.0 æ–°å¢ MOPS + Yahooï¼‰"""
    today = datetime.now().strftime('%Y-%m-%d')
    weekday = ['ä¸€', 'äºŒ', 'ä¸‰', 'å››', 'äº”', 'å…­', 'æ—¥'][datetime.now().weekday()]
    time_now = datetime.now().strftime('%H:%M')

    print("\n" + "=" * 60)
    print(f"ğŸ“¢ å°è‚¡æ™‚äº‹æƒæ v4.0ï¼ˆ{today} é€±{weekday} {time_now}ï¼‰")
    print("=" * 60)

    # ğŸ”´ MOPS é‡å¤§è¨Šæ¯ï¼ˆæœ€é‡è¦ï¼ï¼‰
    print("\nã€ğŸ”´ MOPS é‡å¤§è¨Šæ¯ã€‘ï¼ˆå…¬é–‹è³‡è¨Šè§€æ¸¬ç«™ï¼‰")
    if mops_announcements:
        important = [a for a in mops_announcements if a.get('is_important')]
        others = [a for a in mops_announcements if not a.get('is_important')]

        if important:
            print("  âš ï¸ é‡é»è¨Šæ¯ï¼š")
            for a in important[:8]:
                print(f"    â€¢ {a['stock_name']}({a['stock_code']}): {a['subject'][:45]}...")

        if others:
            print("  ğŸ“‹ å…¶ä»–è¨Šæ¯ï¼š")
            for a in others[:5]:
                print(f"    â€¢ {a['stock_name']}({a['stock_code']}): {a['subject'][:45]}...")
    else:
        print("  ï¼ˆä»Šæ—¥å°šç„¡é‡å¤§è¨Šæ¯æˆ–æŸ¥è©¢å¤±æ•—ï¼‰")

    # ç†±é–€é¡Œæ
    print("\nã€ğŸ”¥ ç†±é–€é¡Œæã€‘")
    if hot_topics:
        for topic, count in hot_topics:
            bar = "â–ˆ" * min(count, 10)
            print(f"  â€¢ {topic}: {bar} ({count}å‰‡)")
    else:
        print("  ï¼ˆåˆ†æä¸­...ï¼‰")

    # åµæ¸¬åˆ°çš„é‡è¦äº‹ä»¶
    print("\nã€â° åµæ¸¬åˆ°çš„äº‹ä»¶ã€‘")
    if events:
        for e in events:
            print(f"  {e['impact']} {e['event']} - å½±éŸ¿ï¼š{e['stocks']}")
            print(f"      ä¾†æºï¼š{e['source_title']}...")
    else:
        print("  ï¼ˆä»Šæ—¥æ–°èæœªåµæ¸¬åˆ°é‡å¤§äº‹ä»¶ï¼‰")

    # åµæ¸¬åˆ°çš„æ³•èªªæœƒ
    print("\nã€ğŸ“… åµæ¸¬åˆ°çš„æ³•èªªæœƒã€‘")
    if conferences:
        for c in conferences:
            print(f"  â­ {c['stock_name']}({c['stock_code']})")
            print(f"      {c['title'][:50]}...")
    else:
        print("  ï¼ˆä»Šæ—¥æ–°èæœªæåŠæ³•èªªæœƒï¼‰")

    # è­‰äº¤æ‰€å®˜æ–¹å…¬å‘Š
    print("\nã€ğŸ“£ è­‰äº¤æ‰€å…¬å‘Šã€‘")
    if twse_announcements:
        for a in twse_announcements[:5]:
            print(f"  â€¢ {a['subject'][:55]}...")
    else:
        print("  ï¼ˆç„¡æœ€æ–°å…¬å‘Šï¼‰")

    # è²¡ç¶“æ–°èï¼ˆæ•´åˆæ‰€æœ‰ä¾†æºï¼‰
    print("\nã€ğŸ“° è²¡ç¶“æ–°èã€‘")
    all_financial_news = cnyes_news + udn_news + (yahoo_news or [])
    if all_financial_news:
        seen = set()
        count = 0
        for n in all_financial_news:
            title = n.get('title', '')[:50]
            if title and title not in seen and count < 12:
                seen.add(title)
                source = n.get('source', '')
                print(f"  [{source}] {title}...")
                count += 1
    else:
        print("  ï¼ˆè¼‰å…¥ä¸­...ï¼‰")

    print("\n" + "=" * 60)
    print("âœ… æƒæå®Œæˆï¼ˆè³‡æ–™ä¾†æºï¼šMOPSã€è­‰äº¤æ‰€ã€é‰…äº¨ç¶²ã€Yahooã€ç¶“æ¿Ÿæ—¥å ±ï¼‰")
    print("=" * 60)

    return {
        'date': today,
        'hot_topics': hot_topics,
        'events': events,
        'conferences': conferences,
        'announcements': twse_announcements,
        'twse_news': twse_news,
        'news': cnyes_news[:10]
    }


def main():
    print("â³ æ­£åœ¨æƒæå°è‚¡æ™‚äº‹ v4.0...")

    # ğŸ”´ MOPS é‡å¤§è¨Šæ¯ï¼ˆæœ€é‡è¦ä¾†æºï¼‰
    print("ğŸ“¡ æŸ¥è©¢ MOPS é‡å¤§è¨Šæ¯ï¼ˆå…¬é–‹è³‡è¨Šè§€æ¸¬ç«™ï¼‰...")
    mops_announcements = get_mops_announcements()
    print(f"   æ‰¾åˆ° {len(mops_announcements)} å‰‡")

    # è­‰äº¤æ‰€
    print("ğŸ“¡ æŸ¥è©¢è­‰äº¤æ‰€æ–°è...")
    twse_news = get_twse_news()
    print(f"   æ‰¾åˆ° {len(twse_news)} å‰‡")

    print("ğŸ“¡ æŸ¥è©¢è­‰äº¤æ‰€å…¬å‘Š...")
    twse_announcements = get_twse_announcements()
    print(f"   æ‰¾åˆ° {len(twse_announcements)} å‰‡")

    # è²¡ç¶“åª’é«”
    print("ğŸ“¡ æŸ¥è©¢é‰…äº¨ç¶²æ–°è...")
    cnyes_news = get_cnyes_news()
    print(f"   æ‰¾åˆ° {len(cnyes_news)} å‰‡")

    print("ğŸ“¡ æŸ¥è©¢ Yahoo è‚¡å¸‚...")
    yahoo_news = get_yahoo_tw_news()
    print(f"   æ‰¾åˆ° {len(yahoo_news)} å‰‡")

    print("ğŸ“¡ æŸ¥è©¢ç¶“æ¿Ÿæ—¥å ±...")
    udn_news = get_udn_rss()
    print(f"   æ‰¾åˆ° {len(udn_news)} å‰‡")

    # æ•´åˆæ‰€æœ‰æ–°èé€²è¡Œåˆ†æï¼ˆåŒ…å« MOPS è¨Šæ¯ï¼‰
    mops_as_news = [{'title': f"{a['stock_name']}: {a['subject']}", 'summary': '', 'source': 'MOPS'}
                    for a in mops_announcements]
    all_news = twse_news + cnyes_news + udn_news + yahoo_news + mops_as_news

    print("ğŸ“¡ åˆ†æç†±é–€é¡Œæ...")
    hot_topics = analyze_hot_topics(all_news)

    print("ğŸ“¡ åµæ¸¬æ³•èªªæœƒ...")
    conferences = detect_conferences(all_news)
    print(f"   åµæ¸¬åˆ° {len(conferences)} å ´")

    print("ğŸ“¡ åµæ¸¬é‡è¦äº‹ä»¶...")
    events = detect_events(all_news)
    print(f"   åµæ¸¬åˆ° {len(events)} å€‹")

    # è¼¸å‡ºæ‘˜è¦ï¼ˆv4.0 æ–°å¢ MOPS + Yahooï¼‰
    result = print_summary(twse_news, twse_announcements, cnyes_news, udn_news,
                          conferences, events, hot_topics,
                          mops_announcements, yahoo_news)

    # è£œå…… MOPS åˆ°çµæœ
    result['mops_announcements'] = mops_announcements
    result['yahoo_news'] = yahoo_news[:10]

    # å„²å­˜çµæœ
    today = datetime.now().strftime('%Y-%m-%d')
    output_dir = f'data/{today}'

    try:
        import os
        os.makedirs(output_dir, exist_ok=True)

        output_file = f'{output_dir}/tw_market_news.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2, default=str)
        print(f"\nğŸ’¾ å·²å„²å­˜è‡³ {output_file}")
    except Exception as e:
        print(f"\nâš ï¸ å„²å­˜å¤±æ•—: {e}")


if __name__ == '__main__':
    main()
